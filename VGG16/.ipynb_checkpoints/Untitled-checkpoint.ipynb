{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, ZeroPadding2D,\n",
    "                          MaxPooling2D, Activation, Dense, Dropout, Flatten)\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"VGG16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_digits():\n",
    "    x = Input(shape = (16, 15, 1))\n",
    "    y = ZeroPadding2D(padding = (8, 9))(x) \n",
    "\n",
    "    y = multiConvLayer(y, 64, 2) # size 32x32\n",
    "    y = multiConvLayer(y, 128, 2) # size 16x16\n",
    "    y = multiConvLayer(y, 256, 3) # size 8x8\n",
    "    y = multiConvLayer(y, 512, 3) # size 4x4\n",
    "    y = multiConvLayer(y, 512, 3) # size 2x2\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(units = 4096, activation='relu')(y)\n",
    "    y = Dense(units = 4096, activation='relu')(y)\n",
    "    y = Dense(units = 10)(y)\n",
    "    y = Activation('softmax')(y)\n",
    "\n",
    "    return Model(x, y, name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiConvLayer(x, value, n):\n",
    "    y = x\n",
    "    for _ in range(n):\n",
    "        y = Conv2D(value, (3, 3), padding = 'same')(y)\n",
    "        y = Activation('relu')(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2), strides = (2, 2))(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDir():\n",
    "    # Used to save the model\n",
    "    # Not implemented yet\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    # Used to save the logs\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "    y_train = to_categorical(y_train) # encode one-hot vector\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    num_of_test_data = 50000\n",
    "    x_val = x_train[num_of_test_data:]\n",
    "    y_val = y_train[num_of_test_data:]\n",
    "    x_train = x_train[:num_of_test_data]\n",
    "    y_train = y_train[:num_of_test_data]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"models/model.h5\")\n",
    "\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # load json and create model\n",
    "    json_file = open('models/model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"models/model.h5\")\n",
    "\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digits():\n",
    "    Data = np.transpose(np.loadtxt(\"Digits_Dataset.txt\"))\n",
    "    Data = Data.transpose()\n",
    "    Data = Data.astype(int)\n",
    "    Data.shape\n",
    "\n",
    "    x = 0\n",
    "    Classes = np.array([])\n",
    "    Classes = Classes.astype(int)\n",
    "\n",
    "    for i in range(1, 2001):\n",
    "        Classes = np.append(Classes, x)\n",
    "        if(i != 0 and i % 200 == 0):\n",
    "            x = x + 1\n",
    "            continue\n",
    "\n",
    "    X_Digits, X_test_Digits, Y_Digits, Y_test_Digits = train_test_split(Data, Classes, test_size=0.2, stratify = Classes, random_state=10)\n",
    "    batch_size = 25\n",
    "    Y_Digits_Total = np.concatenate((Y_Digits, Y_test_Digits), axis = None)\n",
    "\n",
    "    X_Digits = X_Digits / 6.\n",
    "    X_test_Digits = X_test_Digits / 6.\n",
    "\n",
    "    X_Digits = X_Digits.reshape(-1, 16, 15, 1)\n",
    "    X_test_Digits = X_test_Digits.reshape (-1, 16, 15, 1)\n",
    "\n",
    "    Y_Digits = to_categorical(Y_Digits)\n",
    "    Y_test_Digits = to_categorical(Y_test_Digits)\n",
    "\n",
    "    X_train_Digits, X_val_Digits, Y_train_Digits, Y_val_Digits = train_test_split(X_Digits, Y_Digits, test_size = 0.2, random_state=10, stratify = Y_Digits)\n",
    "\n",
    "    return (X_train_Digits, Y_train_Digits), (X_val_Digits, Y_val_Digits), (X_test_Digits, Y_test_Digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-bc5013eb23f9>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-bc5013eb23f9>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    history_MNIST = model.fit(datagen_MNIST.flow((training_data[0], training_data[1], batch_size = batch_size), validation_data=(val_data[0], val_data[1]), steps_per_epoch = training_data[0].shape[0] // batch_size, callbacks = [tensorboard, chkpoint])\u001b[0m\n\u001b[1;37m                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def main(arg):\n",
    "    # Create directories\n",
    "    checkDir()\n",
    "\n",
    "    # Define Adam Optimizer\n",
    "    adam = Adam(lr=1e-4, decay=1e-6)\n",
    "    \n",
    "    # Load dataset\n",
    "    training_data, val_data, test_data = load_digits()\n",
    "\n",
    "    tensorboard = TensorBoard(write_grads=True, write_images=True)\n",
    "    chkpoint = ModelCheckpoint(\"models/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", save_best_only=True)\n",
    "\n",
    "    if arg == \"train\":\n",
    "        model = vgg16_digits()\n",
    "        model.compile(adam, 'categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        datagen_MNIST = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # dimesion reduction\n",
    "                rotation_range=5,  # randomly rotate images in the range 5 degrees\n",
    "                zoom_range = 0.1, # Randomly zoom image 10%\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally 10%\n",
    "                height_shift_range=0.1,  # randomly shift images vertically 10%\n",
    "                horizontal_flip=False,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images\n",
    "\n",
    "        datagen_MNIST.fit(training_data[0])\n",
    "\n",
    "        epochs = 50\n",
    "        batch_size = 25\n",
    "        history_MNIST = model.fit(datagen_MNIST.flow((training_data[0], training_data[1], batch_size = batch_size), validation_data=(val_data[0], val_data[1]), steps_per_epoch = training_data[0].shape[0] // batch_size, callbacks = [tensorboard, chkpoint])\n",
    "\n",
    "        '''\n",
    "        acc_MNIST = history_MNIST.history['accuracy']\n",
    "        val_acc_MNIST = history_MNIST.history['val_accuracy']\n",
    "        loss_MNIST = history_MNIST.history['loss']\n",
    "        val_loss_MNIST = history_MNIST.history['val_loss']\n",
    "        epochs_graph = range(1, len(acc_MNIST) + 1)\n",
    "\n",
    "        plt.title('Training and Validation Accuracy', fontsize = '24')\n",
    "        plt.xlabel(\"Epochs\", fontsize = '24')\n",
    "        plt.ylabel(\"Accuracy\", fontsize = '24')\n",
    "        plt.xticks(fontsize = '20')\n",
    "        plt.yticks(fontsize = '20')\n",
    "        plt.plot(epochs_graph, acc_MNIST, 'red', label='Training accuracy')\n",
    "        plt.plot(epochs_graph, val_acc_MNIST, 'blue', label='Validation accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Training and Validation Loss', fontsize = '24')\n",
    "        plt.xticks(fontsize = '20')\n",
    "        plt.yticks(fontsize = '20')\n",
    "        plt.xlabel(\"Epochs\", fontsize = '24')\n",
    "        plt.ylabel(\"Loss\", fontsize = '24')\n",
    "        plt.plot(epochs_graph, loss_MNIST, 'red', label='Training loss')\n",
    "        plt.plot(epochs_graph, val_loss_MNIST, 'blue', label='Validation loss')\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        Y_pred_MNIST = model.predict(test_data[0])\n",
    "        Y_pred_classes_MNIST = np.argmax(Y_pred_MNIST, axis = 1) \n",
    "        Y_truth_MNIST = np.argmax(test_data[1], axis = 1) \n",
    "\n",
    "        confusion_mtx_MNIST = confusion_matrix(Y_truth_MNIST, Y_pred_classes_MNIST)\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(16, 10))\n",
    "        sns.heatmap(confusion_mtx_MNIST, annot=True, linewidths=0.01, cmap=\"viridis\", linecolor=\"gray\", fmt= '.1f', ax = ax)\n",
    "        plt.xlabel(\"Predicted Label\", fontsize = 24)\n",
    "        plt.ylabel(\"True Label\", fontsize = 24)\n",
    "        plt.xticks(fontsize = '20')\n",
    "        plt.yticks(fontsize = '20')\n",
    "        plt.title(\"Confusion Matrix\", fontsize = '24')\n",
    "        plt.show()\n",
    "        '''\n",
    "        save_model(model)\n",
    "    elif arg == \"test\":\n",
    "        model = load_model()\n",
    "        model.compile(adam, 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        print(\"Invalid argument\")\n",
    "        sys.exit()\n",
    "\n",
    "    score = model.evaluate(test_data[0], test_data[1], verbose=0)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 40 steps, validate on 320 samples\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 2.3022 - accuracy: 0.0984 - val_loss: 2.2870 - val_accuracy: 0.1000\n",
      "Saved model to disk\n",
      "[2.2868150424957276, 0.1]\n"
     ]
    }
   ],
   "source": [
    "main(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Loaded model from disk\n",
      "[0.08542589845950715, 0.9825]\n"
     ]
    }
   ],
   "source": [
    "main(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
